{
  "seed": 123,
  "epochs": 1,
  "max_samples_per_epoch": 100,
  "save_every_n_epochs": 1,
  "reward_mode": "sparse",
  "num_ppo_epochs": 4,
  "whiten_rewards": false,
  "kl_coef": 0.05,
  "cliprange": 0.2,
  "cliprange_value": 0.2,
  "vf_coef": 0.1,
  "gamma": 1.0,
  "lam": 0.95,
  "learning_rate": 1e-05,
  "weight_decay": 0.01,
  "max_grad_norm": 1.0,
  "save_dir": "/kaggle/working/LLM_Alignment/checkpoints/ppo_sparse_seed_123",
  "output_dir": "checkpoints",
  "data_dir": "data/processed",
  "reward_model_path": "/kaggle/working/LLM_Alignment/models/reward_model/final_model",
  "model_name": "HuggingFaceTB/SmolLM2-135M-Instruct",
  "max_length": 512,
  "max_new_tokens": 256,
  "temperature": 0.7,
  "top_k": 50,
  "top_p": 0.95,
  "load_in_8bit": false,
  "load_in_4bit": true,
  "mixed_precision": "fp16",
  "use_lora": true,
  "lora_r": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05
}