{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a02808",
   "metadata": {},
   "source": [
    "1. Setup and Model Loading\n",
    "\n",
    "We will use ResNet-18 (CNN) and ViT-B-16 (Transformer) as our two \"brains\". We also load the CIFAR-10 dataset. Even though these models are trained on ImageNet, CIFAR-10 is fast to download and sufficient to see if they activate for similar concepts (like \"car wheels\" or \"dog ears\").\n",
    "\n",
    "We also define a helper class FeatureExtractor that attaches \"hooks\" to the models to steal their internal thoughts (activations) while images pass through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1a1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Models (ResNet-18 & ViT-B)...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: SETUP & DATA LOADING\n",
    "#Load M distinct models\n",
    "# ==========================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Loading Models (ResNet-18 & ViT-B)...\")\n",
    "resnet = models.resnet18(weights=\"IMAGENET1K_V1\").to(device).eval()\n",
    "vit = models.vit_b_16(weights=\"IMAGENET1K_V1\").to(device).eval()\n",
    "\n",
    "# Helper class to capture activations without blocking gradients\n",
    "class HookManager:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.activations = []\n",
    "        for name, module in model.named_modules():\n",
    "            if name == layer_name:\n",
    "                module.register_forward_hook(self.hook_fn)\n",
    "                break\n",
    "    def hook_fn(self, module, input, output):\n",
    "        # Flatten logic: ResNet=(B,C,H,W)->(B,C), ViT=(B,Seq,D)->(B,D)\n",
    "        if len(output.shape) == 4: flat = output.mean(dim=[2, 3]) \n",
    "        elif len(output.shape) == 3: flat = output[:, 0, :] \n",
    "        else: flat = output\n",
    "        self.activations.append(flat) # Keep gradient flow\n",
    "    def clear(self): self.activations = []\n",
    "\n",
    "# Data Loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "iter_loader = iter(dataloader)\n",
    "\n",
    "# Initial Hooks\n",
    "hk_res = HookManager(resnet, \"layer4\")\n",
    "hk_vit = HookManager(vit, \"encoder.layers.encoder_layer_11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc4f55",
   "metadata": {},
   "source": [
    "2. The Universal Sparse Autoencoder (USAE) Class\n",
    "\n",
    "This is the core math from Page 7 of your assignment. Instead of one encoder, we have a list of encoders (one per model).\n",
    "\n",
    "    Encoder: TopK( W_enc * (x - bias) ).\n",
    "\n",
    "Decoder: Z * W_dec + bias.\n",
    "\n",
    "Universality: The extracted feature Z is shared. It lives in a \"Platonic\" space that both models can understand.\n",
    "\n",
    "What we are doing: This is the core assignment task. We define the Universal Sparse Autoencoder. It has:\n",
    "\n",
    "    Multiple Encoders: One for ResNet, one for ViT. They map different brain signals to a shared language.\n",
    "\n",
    "    Shared Latent Space (Z): A vector of size 2048. We force this to be sparse (mostly zeros) using a TopK activation.\n",
    "\n",
    "    Multiple Decoders: These translate the shared thought Z back into the specific format ResNet or ViT understands. This structure forces the models to agree on concepts (like \"dog\") in the middle layer Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17196307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. MODEL DEFINITION (Squeezed Latent) ---\n",
    "class UniversalSAE(nn.Module):\n",
    "    def __init__(self, input_dims, latent_dim=1024, k=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim \n",
    "        self.k = k\n",
    "        # M Encoders / Decoders\n",
    "        self.encoders = nn.ModuleList([nn.Linear(dim, latent_dim) for dim in input_dims])\n",
    "        self.decoders = nn.ModuleList([nn.Linear(latent_dim, dim, bias=False) for dim in input_dims])\n",
    "        self.b_pre = nn.ParameterList([nn.Parameter(torch.zeros(dim)) for dim in input_dims])\n",
    "        \n",
    "        # Unit Norm Initialization\n",
    "        with torch.no_grad():\n",
    "            for dec in self.decoders:\n",
    "                dec.weight.data = F.normalize(dec.weight.data, p=2, dim=0)\n",
    "\n",
    "    def encode(self, x, model_idx):\n",
    "        # [cite_start]Step (b) Encode [cite: 178]\n",
    "        x_centered = x - self.b_pre[model_idx]\n",
    "        pre_acts = self.encoders[model_idx](x_centered)\n",
    "        vals, inds = torch.topk(pre_acts, self.k, dim=1)\n",
    "        z = torch.zeros_like(pre_acts)\n",
    "        z.scatter_(1, inds, vals)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, model_idx):\n",
    "        # [cite_start]Step (c) Decode [cite: 181]\n",
    "        return self.decoders[model_idx](z) + self.b_pre[model_idx]\n",
    "\n",
    "input_dims = [512, 768] \n",
    "usae = UniversalSAE(input_dims, latent_dim=1024, k=32).to(device)\n",
    "optimizer = optim.Adam(usae.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7270332a",
   "metadata": {},
   "source": [
    "3. Training Loop\n",
    "\n",
    "We strip-mine the data: run images through ResNet/ViT, catch the activations, and train the SAE to reconstruct them. The Universal Trick: We randomly swap which model we are training on. This forces the latent_dim (the shared features) to be useful for both architectures.\n",
    "\n",
    "What we are doing: We run a loop for 1,000 steps. In each step:\n",
    "\n",
    "    We pass a batch of images through ResNet and ViT to get their activations.\n",
    "\n",
    "    We pick one model randomly (e.g., ResNet) and train the SAE to compress and reconstruct its activations.\n",
    "\n",
    "    By randomly switching between ResNet and ViT every step, the shared middle layer (Z) is forced to learn features that work for both. This creates the \"Universal\" representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2df27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training (4000 steps)...\n",
      "Step 0 | Loss: 0.01980\n",
      "Step 500 | Loss: 0.00282\n",
      "Step 1000 | Loss: 0.00241\n",
      "Step 1500 | Loss: 0.00245\n",
      "Step 2000 | Loss: 0.00214\n",
      "Step 2500 | Loss: 0.00199\n",
      "Step 3000 | Loss: 0.00192\n"
     ]
    }
   ],
   "source": [
    "# --- 3. TRAINING LOOP (Longer & Cleaner) ---\n",
    "num_steps = 4000\n",
    "losses = []\n",
    "\n",
    "print(f\"Starting Training ({num_steps} steps)...\")\n",
    "\n",
    "for step in range(num_steps):\n",
    "    try: images, _ = next(iter_loader)\n",
    "    except: iter_loader = iter(dataloader); images, _ = next(iter_loader)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    # 1. Get Activations\n",
    "    hk_res.clear(); hk_vit.clear()\n",
    "    with torch.no_grad(): resnet(images); vit(images)\n",
    "    \n",
    "    # Normalize inputs [Essential for scale alignment]\n",
    "    act_res = F.normalize(hk_res.activations[0], p=2, dim=1)\n",
    "    act_vit = F.normalize(hk_vit.activations[0], p=2, dim=1)\n",
    "    \n",
    "    # [cite_start]2. Step (a): Select Random Source [cite: 176]\n",
    "    src_idx = np.random.randint(0, 2)\n",
    "    src_act = [act_res, act_vit][src_idx]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 3. Step (b): Encode Source -> Z\n",
    "    z = usae.encode(src_act, src_idx)\n",
    "    \n",
    "    # [cite_start]4. Step (c): Universal Decode (Reconstruct ALL models) [cite: 180]\n",
    "    recon_res = usae.decode(z, 0)\n",
    "    recon_vit = usae.decode(z, 1)\n",
    "    \n",
    "    # [cite_start]5. Step (d): Aggregate Loss [cite: 183]\n",
    "    loss_res = nn.MSELoss()(recon_res, act_res)\n",
    "    loss_vit = nn.MSELoss()(recon_vit, act_vit)\n",
    "    \n",
    "    loss = loss_res + loss_vit + (1e-4 * z.abs().sum()) # + Sparsity\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Enforce Unit Norm on Decoder Columns\n",
    "    with torch.no_grad():\n",
    "        for dec in usae.decoders:\n",
    "            dec.weight.data = F.normalize(dec.weight.data, p=2, dim=0)\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if step % 500 == 0: print(f\"Step {step} | Loss: {loss.item():.5f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses); plt.title(\"USAE Aggregate Training Loss\"); plt.savefig(\"Fig1_Loss.png\")\n",
    "print(\"Training Complete. Fig1 saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd6460",
   "metadata": {},
   "source": [
    "4. Quantifying Universality (Firing Entropy)\n",
    "\n",
    "Now we check if the hypothesis is true. We measure \"Firing Entropy\".\n",
    "\n",
    "    If a feature fires 50% for ResNet and 50% for ViT, it is Universal (Entropy ≈1).\n",
    "\n",
    "    If it only fires for ResNet, it is Model-Specific (Entropy ≈0).\n",
    "\n",
    "We will run a fresh batch of data and track who fires which feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a802651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Metrics Analysis...\")\n",
    "\n",
    "fire_counts = torch.zeros(usae.latent_dim, 2).to(device)\n",
    "max_act_res = torch.zeros(usae.latent_dim).to(device)\n",
    "max_act_vit = torch.zeros(usae.latent_dim).to(device)\n",
    "r2_matrix = torch.zeros(2, 2).to(device)\n",
    "co_fire_intersection = 0\n",
    "co_fire_union = 0\n",
    "\n",
    "def get_r2(true, pred):\n",
    "    ss_res = ((true - pred) ** 2).sum()\n",
    "    ss_tot = ((true - true.mean()) ** 2).sum()\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(50): \n",
    "        try: images, _ = next(iter_loader)\n",
    "        except: iter_loader = iter(dataloader); images, _ = next(iter_loader)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        hk_res.clear(); hk_vit.clear()\n",
    "        resnet(images); vit(images)\n",
    "        \n",
    "        act_res = F.normalize(hk_res.activations[0], p=2, dim=1)\n",
    "        act_vit = F.normalize(hk_vit.activations[0], p=2, dim=1)\n",
    "        acts = [act_res, act_vit]\n",
    "        \n",
    "        # 1. Entropy Stats\n",
    "        z_r = usae.encode(act_res, 0)\n",
    "        z_v = usae.encode(act_vit, 1)\n",
    "        \n",
    "        fire_counts[:, 0] += (z_r > 0).float().sum(dim=0)\n",
    "        fire_counts[:, 1] += (z_v > 0).float().sum(dim=0)\n",
    "        max_act_res = torch.max(max_act_res, z_r.max(dim=0)[0])\n",
    "        max_act_vit = torch.max(max_act_vit, z_v.max(dim=0)[0])\n",
    "        \n",
    "        # [cite_start]2. R^2 Matrix [cite: 184]\n",
    "        for src in range(2):\n",
    "            z_shared = usae.encode(acts[src], src)\n",
    "            for tgt in range(2):\n",
    "                recon = usae.decode(z_shared, tgt)\n",
    "                r2_matrix[src, tgt] += get_r2(acts[tgt], recon)\n",
    "                \n",
    "        # [cite_start]3. Co-Firing [cite: 195]\n",
    "        both = ((z_r > 0) & (z_v > 0)).float().sum()\n",
    "        either = ((z_r > 0) | (z_v > 0)).float().sum()\n",
    "        co_fire_intersection += both; co_fire_union += either\n",
    "\n",
    "# Metrics Processing\n",
    "probs = fire_counts / (fire_counts.sum(dim=1, keepdim=True) + 1e-9)\n",
    "entropy = -(probs * torch.log(probs + 1e-9)).sum(dim=1) / np.log(2)\n",
    "r2_matrix /= 50\n",
    "co_firing_prop = co_fire_intersection / (co_fire_union + 1e-9)\n",
    "\n",
    "# [cite_start]Plot Entropy [cite: 193]\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(entropy.cpu().numpy(), bins=50, range=(0,1))\n",
    "plt.title(\"Normalized Firing Entropy\")\n",
    "plt.axvline(x=0.9, color='r', linestyle='--', label='Universal')\n",
    "plt.legend(); plt.savefig(\"Fig2_Entropy.png\")\n",
    "\n",
    "# [cite_start]Plot R^2 Matrix [cite: 184]\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.imshow(r2_matrix.cpu().numpy(), cmap='viridis', vmin=0, vmax=1)\n",
    "plt.colorbar(label='R^2'); plt.title(f\"R^2 Matrix (Co-Fire: {co_firing_prop:.3f})\")\n",
    "plt.xticks([0,1], ['Res', 'ViT']); plt.yticks([0,1], ['Res', 'ViT'])\n",
    "plt.savefig(\"Fig3_R2Matrix.png\")\n",
    "\n",
    "print(f\"METRICS:\\nCo-Firing Proportion: {co_firing_prop:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643c4e1",
   "metadata": {},
   "source": [
    "5. Visualization: Feature Consensus\n",
    "\n",
    "What we are doing: We want to see what the shared features look like.\n",
    "\n",
    "    We pick a \"Universal\" feature (High Entropy).\n",
    "\n",
    "    We generate two images from scratch (random noise).\n",
    "\n",
    "    We optimize Image A to make ResNet activate that feature.\n",
    "\n",
    "    We optimize Image B to make ViT activate that same feature.\n",
    "\n",
    "    If the images look similar (e.g., both look like a curve or a texture), we have proven the models share that concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ece2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PHASE 5: INDEPENDENT SAE (Alignment Tax)\n",
    "# Compare USAE vs Independent SAE\n",
    "# ==========================================\n",
    "print(\"Running Alignment Tax Experiment...\")\n",
    "\n",
    "ind_sae = nn.Sequential(\n",
    "    nn.Linear(512, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 512, bias=False)\n",
    ").to(device)\n",
    "opt_ind = optim.Adam(ind_sae.parameters(), lr=1e-3)\n",
    "\n",
    "for step in range(1000):\n",
    "    try: images, _ = next(iter_loader)\n",
    "    except: iter_loader = iter(dataloader); images, _ = next(iter_loader)\n",
    "    images = images.to(device)\n",
    "    \n",
    "    hk_res.clear()\n",
    "    with torch.no_grad(): resnet(images)\n",
    "    act = F.normalize(hk_res.activations[0], p=2, dim=1)\n",
    "    \n",
    "    opt_ind.zero_grad()\n",
    "    # Manual Top-K simulation\n",
    "    z_pre = ind_sae[0](act)\n",
    "    vals, inds = torch.topk(z_pre, 32, dim=1)\n",
    "    z = torch.zeros_like(z_pre).scatter_(1, inds, vals)\n",
    "    recon = ind_sae[2](z)\n",
    "    \n",
    "    loss = nn.MSELoss()(recon, act)\n",
    "    loss.backward(); opt_ind.step()\n",
    "\n",
    "# Tax Calculation\n",
    "with torch.no_grad():\n",
    "    r2_usae = r2_matrix[0,0].item() # Res->Res (USAE)\n",
    "    \n",
    "    hk_res.clear(); resnet(images)\n",
    "    act = F.normalize(hk_res.activations[0], p=2, dim=1)\n",
    "    z_pre = ind_sae[0](act)\n",
    "    vals, inds = torch.topk(z_pre, 32, dim=1)\n",
    "    z = torch.zeros_like(z_pre).scatter_(1, inds, vals)\n",
    "    recon_ind = ind_sae[2](z)\n",
    "    r2_ind = get_r2(act, recon_ind).item()\n",
    "\n",
    "print(f\"ALIGNMENT TAX: USAE_R2={r2_usae:.4f} | Indep_R2={r2_ind:.4f} | Tax={r2_ind-r2_usae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PHASE 6: VISUALIZATIONS\n",
    "# [cite: 197-205, 211] 3 Universal + 1 Specific\n",
    "# ==========================================\n",
    "print(\"Generating Visualizations...\")\n",
    "\n",
    "# Swap to Gradient-Safe Hooks for Viz\n",
    "resnet.layer4._forward_hooks.clear()\n",
    "vit.encoder.layers.encoder_layer_11._forward_hooks.clear()\n",
    "\n",
    "class VizHook:\n",
    "    def __init__(self, model, layer_name):\n",
    "        self.activations = []\n",
    "        for name, module in model.named_modules():\n",
    "            if name == layer_name:\n",
    "                module.register_forward_hook(self.hook_fn); break\n",
    "    def hook_fn(self, module, input, output):\n",
    "        if len(output.shape) == 4: flat = output.mean(dim=[2, 3]) \n",
    "        elif len(output.shape) == 3: flat = output[:, 0, :] \n",
    "        else: flat = output\n",
    "        self.activations.append(flat) # No detach!\n",
    "    def clear(self): self.activations = []\n",
    "\n",
    "hk_res = VizHook(resnet, \"layer4\")\n",
    "hk_vit = VizHook(vit, \"encoder.layers.encoder_layer_11\")\n",
    "\n",
    "def generate_dream(feature_idx, filename, title):\n",
    "    img_res = F.interpolate(torch.rand(1, 3, 32, 32, device=device), size=(224,224), mode='bilinear').requires_grad_(True)\n",
    "    img_vit = F.interpolate(torch.rand(1, 3, 32, 32, device=device), size=(224,224), mode='bilinear').requires_grad_(True)\n",
    "    opt = optim.Adam([img_res, img_vit], lr=0.05)\n",
    "    \n",
    "    for i in range(150):\n",
    "        opt.zero_grad()\n",
    "        # Jitter\n",
    "        ox, oy = np.random.randint(-3, 4), np.random.randint(-3, 4)\n",
    "        j_res = torch.roll(img_res, shifts=(ox, oy), dims=(2, 3))\n",
    "        j_vit = torch.roll(img_vit, shifts=(ox, oy), dims=(2, 3))\n",
    "        \n",
    "        hk_res.clear(); hk_vit.clear()\n",
    "        resnet(j_res); vit(j_vit)\n",
    "        \n",
    "        act_res = F.normalize(hk_res.activations[0], p=2, dim=1)\n",
    "        act_vit = F.normalize(hk_vit.activations[0], p=2, dim=1)\n",
    "        \n",
    "        # [cite_start]CAM: Maximize Z [cite: 202-203]\n",
    "        score = usae.encode(act_res, 0)[0, feature_idx] + usae.encode(act_vit, 1)[0, feature_idx]\n",
    "        (-score).backward()\n",
    "        \n",
    "        # Blur Gradients\n",
    "        if img_res.grad is not None:\n",
    "            with torch.no_grad():\n",
    "                img_res.grad = TF.gaussian_blur(img_res.grad, 5)\n",
    "                img_vit.grad = TF.gaussian_blur(img_vit.grad, 5)\n",
    "        opt.step()\n",
    "        with torch.no_grad(): img_res.data.clamp_(0, 1); img_vit.data.clamp_(0, 1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    def proc(t): return (t.detach().cpu().squeeze().permute(1,2,0).numpy())\n",
    "    ax[0].imshow(proc(img_res)); ax[0].set_title(f\"ResNet: {title}\"); ax[0].axis('off')\n",
    "    ax[1].imshow(proc(img_vit)); ax[1].set_title(f\"ViT: {title}\"); ax[1].axis('off')\n",
    "    plt.savefig(filename); plt.close()\n",
    "\n",
    "# [cite_start]1. Visualize 3 Universal Features [cite: 198]\n",
    "univ_score = (entropy > 0.8).float() * torch.sqrt(max_act_res * max_act_vit)\n",
    "top3_vals, top3_inds = torch.topk(univ_score, k=3)\n",
    "\n",
    "for i, idx in enumerate(top3_inds):\n",
    "    feat_idx = idx.item()\n",
    "    generate_dream(feat_idx, f\"Fig4_Universal_{i+1}.png\", f\"Universal Feat {feat_idx}\")\n",
    "    print(f\"Saved Fig4_Universal_{i+1}.png\")\n",
    "\n",
    "# [cite_start]2. Visualize 1 Specific Feature [cite: 211]\n",
    "best_spec = torch.argmin(entropy).item()\n",
    "generate_dream(best_spec, \"Fig5_Specific.png\", f\"Specific Feat {best_spec}\")\n",
    "print(f\"Saved Fig5_Specific.png\")\n",
    "\n",
    "print(\"ALL TASKS COMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
