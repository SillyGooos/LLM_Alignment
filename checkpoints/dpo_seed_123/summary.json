{
  "method": "DPO",
  "model_name": "HuggingFaceTB/SmolLM2-135M-Instruct",
  "use_lora": true,
  "lora_r": 8,
  "quantization": "4-bit",
  "seed": 123,
  "training": {
    "batch_size": 16,
    "gradient_accumulation_steps": 4,
    "effective_batch_size": 64,
    "learning_rate": 5e-05,
    "epochs": 1,
    "optimizer": "adamw_torch",
    "beta": 0.1,
    "loss_type": "sigmoid"
  },
  "eval_metrics": {
    "eval_loss": 0.2990253269672394,
    "eval_runtime": 155.3046,
    "eval_samples_per_second": 7.894,
    "eval_steps_per_second": 0.496,
    "eval_rewards/chosen": -1.1511203050613403,
    "eval_rewards/rejected": -2.7244279384613037,
    "eval_rewards/accuracies": 0.9272727370262146,
    "eval_rewards/margins": 1.5733075141906738,
    "eval_logps/chosen": -386.3572998046875,
    "eval_logps/rejected": -483.47686767578125,
    "eval_logits/chosen": 6.167160511016846,
    "eval_logits/rejected": 5.838801383972168,
    "epoch": 1.0
  },
  "perplexity": {
    "perplexity": 16.749900828956324,
    "avg_loss": 2.8183923375942244,
    "num_examples": 1000,
    "total_tokens": 306664
  },
  "kl_divergence": {
    "kl_mean": -0.30738543629646303,
    "kl_std": 0.0837642823111851,
    "kl_min": -0.5393481254577637,
    "kl_max": -0.08189836144447327,
    "kl_median": -0.29809342324733734,
    "num_samples": 100
  }
}